{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheJojoJoseph/M24DE3041_MLBD_assignment4/blob/main/M24DE3041_assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6132f170",
      "metadata": {
        "id": "6132f170"
      },
      "source": [
        " Assignment 4 (M24DE4041)\n",
        "\n",
        "implement clustering, a web search engine, and PageRank. Key files include spambase.data, webpages/, and small.txt, which must be correctly placed. The environment assumes a working local Spark setup. Libraries such as pyspark, numpy, and standard Python utilities (random, re, os, time) are used for processing and analysis.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4985b5",
      "metadata": {
        "id": "bd4985b5"
      },
      "source": [
        "# Part 1: Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19094e56",
      "metadata": {
        "id": "19094e56"
      },
      "source": [
        "Part 1: Clustering\n",
        "40 Marks\n",
        "For this part of the algorithm, you have to study two new algorithms called Farthest First\n",
        "algorithm for k center clustering and k-means++.\n",
        "Reading for Farthest First Algorithm:\n",
        "\n",
        "Papers and reading for K-Means++:\n",
        "\n",
        "There are other tons of material available online in the form of slides, YouTube videos which\n",
        "you can access to get a better understanding of the two algorithms.\n",
        "Dataset is included in the folder: Q1- UCI Spam clustering\n",
        "For this assignment we will work on points in Euclidean space represented by vectors of\n",
        "numbers. The dataset is made of 4601 points with 58-dimensions which are features\n",
        "available to an email spam detection system.\n",
        "In Spark, the points can be represented as instances of the class\n",
        "org.apache.spark.mllib.linalg.Vector and can be manipulated through static methods\n",
        "Complete the coding exercises in a Jupyter notebook.\n",
        "Submit your work as a .ipynb file or PDF file.\n",
        "Ensure all code is properly commented and executed before submission.\n",
        "Plagiarism detected will result in zero marks\n",
        "Max marks: 120\n",
        "\n",
        "http://www.wikiwand.com/en/Farthest-first_traversal\n",
        "\n",
        "https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf\n",
        "http://theory.stanford.edu/~sergei/slides/BATS-Means.pdf\n",
        "\n",
        "offered by the class org.apache.spark.mllib.linalg.Vectors (these classes are available\n",
        "both for Java and Python). For example, method Vectors.dense(x) transforms an array x\n",
        "of double into an instance of class Vector, while method Vectors.sqdist(a,b) computes\n",
        "the squared L2-distance between two instances a and b of class Vector.\n",
        "You must develop 4 functions that are Spark runnable:\n",
        "\n",
        "Make sure that kcenter(P,k) and kmeansPP(P,k) run in time O(|P| ∗ k)\n",
        "Finally, you must create a python program, which receives in input a set P of features\n",
        "(provided as a text file as the above dataset), and 2 integers k, k1, with k < k1. The program\n",
        "incorporates the functions developed above and does the following:\n",
        "\n",
        "Part-2: Web-search\n",
        "40 Marks\n",
        "1. A function readVectorsSeq(filename) that, given in input the name (or path) filename\n",
        "of a text file containing the feature points (one point per line with features separated by\n",
        "comma, as in the file above) transforms it into a list of Vector.\n",
        "2. A function kcenter(P,k) that receives in input a set of points P and an integer k, and\n",
        "returns the set C of k centers computed by the Farthest-First Traversal algorithm. You\n",
        "can represent both P and C as lists of Vector.\n",
        "3. A function kmeansPP(P,k) that receives in input a set of points P and an integer k, and\n",
        "returns a set C of k centers computed with the kmeans++ algorithm.\n",
        "4. A function kmeansObj(P,C) that receives in input a set of points P and a set of centers\n",
        "C, and returns the average squared distance of a point of P from its closest center (i.e.,\n",
        "the kmeans objective function for P with centers C, divided by the number of points of P).\n",
        "\n",
        "1. Runs kcenter(P,k) printing its running time.\n",
        "2. Runs kmeansPP(P,k) to obtain a set of k centers C, and then runs kmeansOb(P,C)\n",
        "printing the returned value\n",
        "3. Runs kcenter(P,k1) to obtain a set of k1 centers X; then runs kmeansPP(X,k) to\n",
        "extarct a set of k centers C from X, and finally runs kmeansObj(P,C) printing the\n",
        "returned value. Here the idea is to test whether k1 > k centers extracted with the kcenter\n",
        "primitive can provide a good coreset on which running kmeans++. Of course, the larger\n",
        "k1 and the better the set of centers computed by kmeansPP(X,k) ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install Spark (only run once)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4gbvF_cJ4_k",
        "outputId": "356423c9-8058-4139-a37e-4ef3b8584582"
      },
      "id": "i4gbvF_cJ4_k",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.3.1-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "69e79665",
      "metadata": {
        "id": "69e79665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4084bca7-296a-467e-c694-c7c4968eb683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------CHEEEEKKKK--- Spark successfully started!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "print(\"------CHEEEEKKKK--- Spark successfully started!\")\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark import SparkContext\n",
        "import random\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.linalg import Vector\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb41efdf",
      "metadata": {
        "id": "eb41efdf"
      },
      "outputs": [],
      "source": [
        "# Transforms feature points into a list of Vector.\n",
        "def readVectorsSeq(filename):\n",
        "    sc = SparkContext.getOrCreate()\n",
        "    lines = sc.textFile(filename)\n",
        "    vectors = lines.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(',')]))\n",
        "    return vectors\n",
        "\n",
        "def squared_distance(a, b):\n",
        "    return sum((ai - bi) ** 2 for ai, bi in zip(a, b))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff1a39f",
      "metadata": {
        "id": "aff1a39f"
      },
      "source": [
        "# kcenter(P,k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542ff1e5",
      "metadata": {
        "id": "542ff1e5"
      },
      "outputs": [],
      "source": [
        "#receives in input a set of points P and an integer k, and returns the set C of k centers computed by the Farthest-First Traversal algorithm.\n",
        "def kcenter(P, k):\n",
        "    centers = []\n",
        "    data = P.collect()\n",
        "    first_center = random.choice(data)\n",
        "    centers.append(first_center)\n",
        "    for _ in range(1, k):\n",
        "        max_dist = -1\n",
        "        next_center = None\n",
        "        for p in data:\n",
        "            min_dist = min(squared_distance(p, c) for c in centers)\n",
        "            if min_dist > max_dist:\n",
        "                max_dist = min_dist\n",
        "                next_center = p\n",
        "        centers.append(next_center)\n",
        "    return centers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13133828",
      "metadata": {
        "id": "13133828"
      },
      "source": [
        "# kmeansPP(P,k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69a8202",
      "metadata": {
        "id": "e69a8202"
      },
      "outputs": [],
      "source": [
        "# receives in input a set of points P and an integer k, and returns a set C of k centers computed with the kmeans++ algorithm.\n",
        "def kmeansPP(P, k):\n",
        "    data = P.collect()\n",
        "    centers = [random.choice(data)]\n",
        "    for _ in range(1, k):\n",
        "        distances = []\n",
        "        for p in data:\n",
        "            min_dist_sq = min(squared_distance(p, c) for c in centers)\n",
        "            distances.append(min_dist_sq)\n",
        "        total = sum(distances)\n",
        "        probabilities = [d / total for d in distances]\n",
        "        next_center = random.choices(data, weights=probabilities, k=1)[0]\n",
        "        centers.append(next_center)\n",
        "    return centers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ae2a72",
      "metadata": {
        "id": "a3ae2a72"
      },
      "source": [
        "# kmeansObj(P, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9980b728",
      "metadata": {
        "id": "9980b728"
      },
      "outputs": [],
      "source": [
        "# receives in input a set of points P and a set of centers C, and returns the average squared distance of a point of P from its closest center\n",
        "def kmeansObj(P, C):\n",
        "    def closest_dist(p):\n",
        "        return min(squared_distance(p, c) for c in C)\n",
        "    total_distance = P.map(closest_dist).sum()\n",
        "    return total_distance / P.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "50b259f1",
      "metadata": {
        "id": "50b259f1",
        "outputId": "2e0e5012-1678-4784-f202-b08383d1e17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data from /content/spambase.data...\n",
            "\n",
            "Running kcenter(P, 10)...\n",
            "Time taken by kcenter: 0.0000 seconds\n",
            "\n",
            "Running kmeansPP(P, 10) and evaluating objective...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 16) (3d147951e6ea executor driver): java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 20 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7aff8d4814d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mk1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-7aff8d4814d2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(filename, k, k1)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nRunning kmeansPP(P, {k}) and evaluating objective...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mC_kmeanspp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeansPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mobj_kmeanspp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeansObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_kmeanspp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Objective value (kmeans++): {obj_kmeanspp:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2b5da44da292>\u001b[0m in \u001b[0;36mkmeansPP\u001b[0;34m(P, k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# receives in input a set of points P and an integer k, and returns a set C of k centers computed with the kmeans++ algorithm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkmeansPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 16) (3d147951e6ea executor driver): java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 20 more\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Set Spark to use the current Python executable\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "\n",
        "def download_file(url, local_filename):\n",
        "    print(f\"Downloading from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise error if the download fails\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"File saved to {local_filename}\")\n",
        "    return local_filename\n",
        "\n",
        "def main(filename, k, k1):\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    print(f\"Reading data from {filename}...\")\n",
        "    P = readVectorsSeq(filename)  # Your custom function to read the file\n",
        "\n",
        "    print(f\"\\nRunning kcenter(P, {k})...\")\n",
        "    start_time = time.time()\n",
        "    end_time = time.time()\n",
        "    print(f\"Time taken by kcenter: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    print(f\"\\nRunning kmeansPP(P, {k}) and evaluating objective...\")\n",
        "    C_kmeanspp = kmeansPP(P, k)\n",
        "    obj_kmeanspp = kmeansObj(P, C_kmeanspp)\n",
        "    print(f\"Objective value (kmeans++): {obj_kmeanspp:.4f}\")\n",
        "\n",
        "    print(f\"\\nRunning kcenter(P, {k1}) to build coreset...\")\n",
        "    X = kcenter(P, k1)\n",
        "    print(f\"Running kmeansPP(X, {k}) on coreset and evaluating objective...\")\n",
        "    C_from_coreset = kmeansPP(sc.parallelize(X), k)\n",
        "    obj_coreset = kmeansObj(P, C_from_coreset)\n",
        "    print(f\"Objective value (coreset): {obj_coreset:.4f}\")\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_url = \"https://raw.githubusercontent.com/TheJojoJoseph/M24DE3041_MLBD_assignment4/858d1d4e905563b50de8d921ba1f315cce2b9b42/Q1-%20UCI%20Spam%20clustering/spambase.data\"\n",
        "    file_path = \"/content/spambase.data\"\n",
        "    # downloaded_file = download_file(raw_url, local_file)\n",
        "    k = 10\n",
        "    k1 = 50\n",
        "    main(file_path, k, k1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61e3c36",
      "metadata": {
        "id": "c61e3c36"
      },
      "source": [
        "# Part 2: Web Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bc9abf",
      "metadata": {
        "id": "c9bc9abf"
      },
      "source": [
        "Part-2: Web-search\n",
        "40 Marks\n",
        "1. A function readVectorsSeq(filename) that, given in input the name (or path) filename\n",
        "of a text file containing the feature points (one point per line with features separated by\n",
        "comma, as in the file above) transforms it into a list of Vector.\n",
        "2. A function kcenter(P,k) that receives in input a set of points P and an integer k, and\n",
        "returns the set C of k centers computed by the Farthest-First Traversal algorithm. You\n",
        "can represent both P and C as lists of Vector.\n",
        "3. A function kmeansPP(P,k) that receives in input a set of points P and an integer k, and\n",
        "returns a set C of k centers computed with the kmeans++ algorithm.\n",
        "4. A function kmeansObj(P,C) that receives in input a set of points P and a set of centers\n",
        "C, and returns the average squared distance of a point of P from its closest center (i.e.,\n",
        "the kmeans objective function for P with centers C, divided by the number of points of P).\n",
        "\n",
        "1. Runs kcenter(P,k) printing its running time.\n",
        "2. Runs kmeansPP(P,k) to obtain a set of k centers C, and then runs kmeansOb(P,C)\n",
        "printing the returned value\n",
        "3. Runs kcenter(P,k1) to obtain a set of k1 centers X; then runs kmeansPP(X,k) to\n",
        "extarct a set of k centers C from X, and finally runs kmeansObj(P,C) printing the\n",
        "returned value. Here the idea is to test whether k1 > k centers extracted with the kcenter\n",
        "primitive can provide a good coreset on which running kmeans++. Of course, the larger\n",
        "k1 and the better the set of centers computed by kmeansPP(X,k) .\n",
        "\n",
        "In this part we will build the basic data structure underlying search engines: an inverted\n",
        "index. We will use this inverted index to answer some simple search queries.\n",
        "An inverted index for a set of webpages\n",
        "Suppose we are given a set of webpages W. For our purposes, each webpage w ∈ W will\n",
        "be considered to be a sequence of words w1, w2, . . . wk. Another way of representing the\n",
        "webpage could be to maintain a list of words along with the position(s) of the words in the\n",
        "webpage. For example consider a web page with the following text:\n",
        "Data structures is the study of structures for storing data.\n",
        "This can be represented as:\n",
        "{(data : 1, 10), (structures : 2, 7), (study : 5), (storing : 9)}\n",
        "Note that the small connector words like “is”, “the”, “of”, “for” have not been stored. Words\n",
        "like this are referred to as stop words and are generally removed since they are very\n",
        "frequent and normally contain no information about the content of the webpage.\n",
        "This representation of the webpage is similar to the index we see at the back of many books\n",
        "which tell us the page numbers where certain important terms used in the book may be\n",
        "found. In fact, we can refer to this as an index for the webpage. In mathematical notation we\n",
        "would say that given a webpage w = w1, w2, . . . , wk, the index of w is:\n",
        "{u : i1(u), . . . ,il(u)) : wij(u) = u, 1 ≤ j ≤ l}\n",
        "\n",
        "An index is used to find the location of a particular string (word) in a specific document or\n",
        "webpage, but when we move to a collection of webpages, we need to first figure out which of\n",
        "the web pages contain the string. For this we store an inverted index. Let us try to define an\n",
        "inverted index formally.\n",
        "Let us suppose we are given a collection of webpages. For each page p ∈ C, let us denote\n",
        "by W(p) the set of all words (excluding stop words) that occur in p. Note that\n",
        "\n",
        "W(C) = ⋃\n",
        "p∈C\n",
        "W(p)\n",
        "\n",
        "is the set of all words in our collection.\n",
        "An inverted index for C will contain an entry for each word w ∈ W. This entry will contain\n",
        "tuples of the form (p, k) to indicate that w occurs in the k\n",
        "\n",
        "th position of page p ∈ C. Using the\n",
        "\n",
        "notation that p[k] denotes the k\n",
        "\n",
        "th word of page p, we can say that the inverted index of C is\n",
        "\n",
        "defined as\n",
        "\n",
        "Inv(C) = {(w : {(p, k) : p ∈ C, p[k] = w}) : w ∈ W(C)}\n",
        "For example, consider the following (small) collection of documents.\n",
        "\n",
        "The inverted index for this would be\n",
        "\n",
        "The web search problem\n",
        "The web search problem is defined as follows:\n",
        "Given a collection of webpages C and a sequence of words q1. . . qk, find the “most relevant”\n",
        "set of pages p1, p2, . . . pA that contain as many of q1. . . qk as possible and return them in the\n",
        "order of decreasing “relevance.”\n",
        "The question of how to measure the relevance of a webpage to a particular query is an\n",
        "involved question with no easy answers. However, for the purpose of this assignment we will\n",
        "work with a simple scoring function.\n",
        "A scoring function for search term relevance\n",
        "One of the simplest scoring function is term frequency-inverse document frequency. It is\n",
        "used to measure how important a word w is to a webpage p. It is a product of two factors i.e.\n",
        "term frequency and inverse document frequency. Given a word w and a webpage p, the\n",
        "relevance score is defined as\n",
        "\n",
        "relevancew(p) = tfw(p) × idfw(p)\n",
        "\n",
        "Term Frequency: It is the total number of occurrence of a word w in a webpage p, denoted by\n",
        "fw(p). It is normalized by the total number of words in webpage p, denoted by |W(p)|. It is\n",
        "defined as\n",
        "\n",
        "tfw(p) =\n",
        "fw(p)\n",
        "|W(p)|\n",
        "1. Data structures is the study of structures for storing data.\n",
        "2. Structural engineers collect data about structures\n",
        "\n",
        "{(data : {(1,1), (1,10), (2,4)}),\n",
        "(structures: {(1,2), (1,7), (2,6)}),\n",
        "(study : {(1,5)}),\n",
        "(storing : {(1,9)}),\n",
        "(structural : {(2,1)}),\n",
        "(engineers : {(2,2)}),\n",
        "(collect : {(2,3)}) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "86101f88",
      "metadata": {
        "id": "86101f88"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class MySet:\n",
        "    def __init__(self):\n",
        "        self.elements = set()\n",
        "\n",
        "    def addElement(self, element):\n",
        "        self.elements.add(element)\n",
        "\n",
        "    def union(self, otherSet):\n",
        "        result = MySet()\n",
        "        result.elements = self.elements.union(otherSet.elements)\n",
        "        return result\n",
        "\n",
        "    def intersection(self, otherSet):\n",
        "        result = MySet()\n",
        "        result.elements = self.elements.intersection(otherSet.elements)\n",
        "        return result\n",
        "\n",
        "    def getElements(self):\n",
        "        return list(self.elements)\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        return item in self.elements\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.elements)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.elements)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.elements)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bede3b3",
      "metadata": {
        "id": "8bede3b3"
      },
      "source": [
        "# class Position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "67838a4b",
      "metadata": {
        "id": "67838a4b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Position:\n",
        "    def __init__(self, pageEntry, wordIndex):\n",
        "        self.pageEntry = pageEntry\n",
        "        self.wordIndex = wordIndex\n",
        "    def getPageEntry(self):\n",
        "        return self.pageEntry\n",
        "    def getWordIndex(self):\n",
        "        return self.wordIndex\n",
        "    def __repr__(self):\n",
        "        return f\"({self.pageEntry.name}, {self.wordIndex})\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "0bfc91b9",
      "metadata": {
        "id": "0bfc91b9"
      },
      "outputs": [],
      "source": [
        "class WordEntry:\n",
        "    def __init__(self, word):\n",
        "        self.word = word\n",
        "        self.positions = []\n",
        "\n",
        "    def addPosition(self, position):\n",
        "        self.positions.append(position)\n",
        "\n",
        "    def addPositions(self, positions):\n",
        "        self.positions.extend(positions)\n",
        "\n",
        "    def getAllPositions(self):\n",
        "        return self.positions\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.word}: {self.positions}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2937139e",
      "metadata": {
        "id": "2937139e"
      },
      "outputs": [],
      "source": [
        "class PageIndex:\n",
        "    def __init__(self):\n",
        "        self.word_entries = {}\n",
        "\n",
        "    def addPositionForWord(self, word, position):\n",
        "        if word in self.word_entries:\n",
        "            self.word_entries[word].addPosition(position)\n",
        "        else:\n",
        "            entry = WordEntry(word)\n",
        "            entry.addPosition(position)\n",
        "            self.word_entries[word] = entry\n",
        "\n",
        "    def getWordEntry(self, word):\n",
        "        return self.word_entries.get(word)\n",
        "\n",
        "    def getAllWordEntries(self):\n",
        "        return self.word_entries.values()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "aaacdb08",
      "metadata": {
        "id": "aaacdb08"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PageEntry:\n",
        "    CONNECTOR_WORDS = {\n",
        "        \"a\", \"an\", \"the\", \"they\", \"these\", \"this\", \"for\", \"is\", \"are\", \"was\", \"of\", \"or\",\n",
        "        \"and\", \"does\", \"will\", \"whose\"\n",
        "    }\n",
        "    PUNCTUATION = r\"[{}\\[\\]<>!=()\\-.,;:'\\\"?#]\"\n",
        "    PLURAL_NORMALIZATION = {\n",
        "        \"structures\": \"structure\", \"structure\": \"structure\",\n",
        "        \"applications\": \"application\", \"application\": \"application\",\n",
        "        \"stacks\": \"stack\", \"stack\": \"stack\"\n",
        "    }\n",
        "    def __init__(self, pageName):\n",
        "        self.name = pageName\n",
        "        self.page_index = PageIndex()\n",
        "        self.readAndProcessPage(pageName)\n",
        "\n",
        "    def normalizeWord(self, word):\n",
        "        word = word.lower()\n",
        "        word = re.sub(PageEntry.PUNCTUATION, ' ', word)\n",
        "        word = word.strip()\n",
        "        if word in PageEntry.PLURAL_NORMALIZATION:\n",
        "            word = PageEntry.PLURAL_NORMALIZATION[word]\n",
        "        return word\n",
        "\n",
        "    def readAndProcessPage(self, pageName):\n",
        "        try:\n",
        "            with open(\"{pageName}\", 'r') as file:\n",
        "                text = file.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Page '{pageName}' not found!\")\n",
        "            self.name = None\n",
        "            return\n",
        "\n",
        "        words = re.split(r'\\s+', text)\n",
        "        index = 1\n",
        "        for word in words:\n",
        "            cleaned = self.normalizeWord(word)\n",
        "            if cleaned and cleaned not in PageEntry.CONNECTOR_WORDS:\n",
        "                pos = Position(self, index)\n",
        "                self.page_index.addPositionForWord(cleaned, pos)\n",
        "            index += 1\n",
        "\n",
        "    def getPageIndex(self):\n",
        "        return self.page_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5645e755",
      "metadata": {
        "id": "5645e755"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyHashTable:\n",
        "    def __init__(self):\n",
        "        self.table = {}\n",
        "    def addPositionsForWord(self, wordEntry):\n",
        "        word = wordEntry.word\n",
        "        if word in self.table:\n",
        "            self.table[word].addPositions(wordEntry.getAllPositions())\n",
        "        else:\n",
        "            self.table[word] = wordEntry\n",
        "    def getWordEntry(self, word):\n",
        "        return self.table.get(word)\n",
        "    def contains(self, word):\n",
        "        return word in self.table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "41be7682",
      "metadata": {
        "id": "41be7682"
      },
      "outputs": [],
      "source": [
        "\n",
        "class InvertedPageIndex:\n",
        "    def __init__(self):\n",
        "        self.pages = []\n",
        "        self.hashtable = MyHashTable()\n",
        "    def addPage(self, pageEntry):\n",
        "        self.pages.append(pageEntry)\n",
        "        for word_entry in pageEntry.getPageIndex().getAllWordEntries():\n",
        "            self.hashtable.addPositionsForWord(word_entry)\n",
        "    def getPagesWhichContainWord(self, word):\n",
        "        word = word.lower()\n",
        "        entry = self.hashtable.getWordEntry(word)\n",
        "        result = MySet()\n",
        "        if entry:\n",
        "            for pos in entry.getAllPositions():\n",
        "                result.addElement(pos.getPageEntry())\n",
        "        return result\n",
        "    def getAllPositionsForWord(self, word):\n",
        "        word = word.lower()\n",
        "        entry = self.hashtable.getWordEntry(word)\n",
        "        return entry.getAllPositions() if entry else []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2aeacd",
      "metadata": {
        "id": "7f2aeacd"
      },
      "source": [
        "# class SearchEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e425764d",
      "metadata": {
        "id": "e425764d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SearchEngine:\n",
        "    def __init__(self):\n",
        "        self.index = InvertedPageIndex()\n",
        "\n",
        "    def performAction(self, actionMessage):\n",
        "        tokens = actionMessage.strip().split()\n",
        "        if not tokens:\n",
        "            return\n",
        "\n",
        "        action = tokens[0]\n",
        "\n",
        "        if action == \"addPage\" and len(tokens) == 2:\n",
        "            pageName = tokens[1]\n",
        "            pageEntry = PageEntry(pageName)\n",
        "            if pageEntry.name is not None:\n",
        "                self.index.addPage(pageEntry)\n",
        "                print(f\"Page '{pageName}' added.\")\n",
        "\n",
        "        elif action == \"queryFindPagesWhichContainWord\" and len(tokens) == 2:\n",
        "            word = tokens[1].lower()\n",
        "            pages = self.index.getPagesWhichContainWord(word)\n",
        "            if len(pages) == 0:\n",
        "                print(f\"No webpage contains word {word}\")\n",
        "            else:\n",
        "                result = sorted(p.name for p in pages)\n",
        "                print(\", \".join(result))\n",
        "\n",
        "        elif action == \"queryFindPositionsOfWordInAPage\" and len(tokens) == 3:\n",
        "            word = tokens[1].lower()\n",
        "            pageName = tokens[2]\n",
        "            positions = self.index.getAllPositionsForWord(word)\n",
        "            found = False\n",
        "            for pos in positions:\n",
        "                if pos.getPageEntry().name == pageName:\n",
        "                    found = True\n",
        "                    print(f\"Word '{word}' found at position(s): {pos.getWordIndex()} in page {pageName}\")\n",
        "            if not found:\n",
        "                if any(pos.getPageEntry().name == pageName for pos in positions):\n",
        "                    print(f\"Webpage {pageName} does not contain word {word}\")\n",
        "                else:\n",
        "                    print(f\"No webpage {pageName} found.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Unknown or malformed command: {actionMessage}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4001cdd0",
      "metadata": {
        "id": "4001cdd0"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "3b4fb8a2",
      "metadata": {
        "id": "3b4fb8a2",
        "outputId": "a4043022-45c3-4b1d-cdb6-3f2b297537d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> addPage stack_datastructure_wiki\n",
            "Page 'stack_datastructure_wiki' not found!\n",
            "\n",
            ">> queryFindPagesWhichContainWord delhi\n",
            "No webpage contains word delhi\n",
            "\n",
            ">> queryFindPagesWhichContainWord stack\n",
            "No webpage contains word stack\n",
            "\n",
            ">> queryFindPagesWhichContainWord wikipedia\n",
            "No webpage contains word wikipedia\n",
            "\n",
            ">> queryFindPositionsOfWordInAPage magazines stack_datastructure_wiki\n",
            "No webpage stack_datastructure_wiki found.\n",
            "\n",
            ">> queryFindPagesWhichContainWord allain\n",
            "No webpage contains word allain\n",
            "\n",
            ">> addPage stack_cprogramming\n",
            "Page 'stack_cprogramming' not found!\n",
            "\n",
            ">> queryFindPagesWhichContainWord allain\n",
            "No webpage contains word allain\n",
            "\n",
            ">> queryFindPagesWhichContainWord C\n",
            "No webpage contains word c\n",
            "\n",
            ">> queryFindPagesWhichContainWord C++\n",
            "No webpage contains word c++\n",
            "\n",
            ">> addPage stack_oracle\n",
            "Page 'stack_oracle' not found!\n",
            "\n",
            ">> queryFindPagesWhichContainWord jdk\n",
            "No webpage contains word jdk\n",
            "\n",
            ">> addPage stackoverflow\n",
            "Page 'stackoverflow' not found!\n",
            "\n",
            ">> queryFindPagesWhichContainWord function\n",
            "No webpage contains word function\n",
            "\n",
            ">> addPage stacklighting\n",
            "Page 'stacklighting' not found!\n",
            "\n",
            ">> addPage stackmagazine\n",
            "Page 'stackmagazine' not found!\n",
            "\n",
            ">> queryFindPagesWhichContainWord magazines\n",
            "No webpage contains word magazines\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    engine = SearchEngine()\n",
        "    actions_file = r\"actions.txt\"\n",
        "    with open(actions_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                print(f\"\\n>> {line.strip()}\")\n",
        "                engine.performAction(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a5b296",
      "metadata": {
        "id": "c5a5b296"
      },
      "source": [
        "# Part 3: PageRank using Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f19581",
      "metadata": {
        "id": "92f19581"
      },
      "source": [
        "Part 3: PageRank on Spark\n",
        "40 Marks\n",
        "Get datasets from : https://github.com/dipsankarb/graphs.git\n",
        "In this assignment you will be implementing PageRank algorithm on Spark. You will be\n",
        "working with a small random graph that is provided. The graph has n=1000 nodes, and\n",
        "described later.\n",
        "\n",
        "addPage x : Add webpage x to the search engine database. The contents of the\n",
        "webpage are stored in a file named x in the webpages folder.\n",
        "queryFindPagesWhichContainWord x : Print the name of the webpages which contain\n",
        "the word x. The list of webpage names should be comma separated. If the word is not\n",
        "found in any webpage, then print “No webpage contains word x”\n",
        "queryFindPositionsOfWordInAPage x y : Print the word indices where the word x is\n",
        "found in the document y. The word indices should be separated by a comma. If the word\n",
        "x is not found in webpage y, then print “Webpage y does not contain word x”. If the\n",
        "webpage is not added in database, then print “No webpage y found”.\n",
        "\n",
        "Convert each word to lowercase.\n",
        "Do not store the connector words in the search engine. However, consider them when\n",
        "you calculate the word indices. Here is a list of connector words: { a, an, the, they, these,\n",
        "this, for, is, are, was, of, or, and, does, will, whose }.\n",
        "Replace the punctuation marks with a space. Here is a list of punctuations: { } [ ] < > = ( )\n",
        ". , ; ’ ” ? # ! - :\n",
        "Plural and singular form: Assume that these words are same: (stack, stacks), (structure,\n",
        "structures), (application, applications).\n",
        "We have given you a set of connector words, punctuation marks, and singular-plural\n",
        "entries. Consider these list of words as exhaustive. You do not need to make any more\n",
        "exceptions in your code.\n",
        "actions.txt contains some simple search queries.\n",
        "answers.txt contains the answers to actions.txt for you to check your code.\n",
        "\n",
        "m=8192 edges. 1000 of these edges form a directed cycle which ensures that the graph is\n",
        "connected. It is trivial to see that the existence of such a cycle ensures that there are no\n",
        "dead ends in the graph. The dataset may have multiple directed edges between a pair of\n",
        "nodes which should be treated as a single edge. The first column in the provided dataset\n",
        "whole.txt is the source vertex and the second column denotes the destination.\n",
        "Assume a directed graph G = (V , E) has n nodes and m edges and all nodes having\n",
        "positive out-degrees, then M is a matrix which is an n × n matrix as defined such that for any\n",
        "(i, j) ∈ [1, n]:\n",
        "\n",
        "Mi,j = {\n",
        "\n",
        "Here, deg(i) is out-degree of the node i. If there are multiple outgoing edges, then treat them\n",
        "as a single node. By definition of PageRank, assuming 1 − β as the teleport probability, and\n",
        "denoting the PageRank vector by the column vector r, we have the following relation:\n",
        "\n",
        "r =\n",
        "1 − β\n",
        "n\n",
        "A + βMr\n",
        "\n",
        "Here, A is the n × 1 unit vector. Based on these equations, the iterative PageRank works as\n",
        "follows:\n",
        "\n",
        "Run this experiment on Spark for 40 iterations (assuming β = 0.8) and obtain the PageRank\n",
        "vector r. In particular, process the matrix M as an RDD and then report the following:\n",
        "\n",
        "Before running the experiment on the whole graph on the server, run the code on a smaller\n",
        "part of the graph given in small.txt with 53 nodes. The top most score in this small graph is\n",
        "0.036"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ae5e3a",
      "metadata": {
        "id": "50ae5e3a"
      },
      "source": [
        "Small.txt dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "20959860",
      "metadata": {
        "id": "20959860",
        "outputId": "5c84cf6a-7d89-46bc-92b5-efd3cd1b5da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 18) (3d147951e6ea executor driver): java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 25 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-38622743fb51>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-38622743fb51>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"small.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mranks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfinal_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-38622743fb51>\u001b[0m in \u001b[0;36minitialize_ranks\u001b[0;34m(links)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitialize_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \"\"\"\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \"\"\"\n\u001b[0;32m-> 2291\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2292\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 18) (3d147951e6ea executor driver): java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Cannot run program \"D:/AndroidStudio/envs/spark/python.exe\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:239)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n\t... 25 more\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "def load_graph(file_path):\n",
        "    sc = SparkContext.getOrCreate()\n",
        "    lines = sc.textFile(file_path)\n",
        "    edges = lines.map(lambda line: tuple(map(int, line.strip().split()))).distinct()\n",
        "    links = edges.groupByKey().mapValues(list).cache()\n",
        "    return sc, links\n",
        "\n",
        "def initialize_ranks(links):\n",
        "    n = links.count()\n",
        "    ranks = links.mapValues(lambda _: 1.0 / n)\n",
        "    return ranks, n\n",
        "\n",
        "def run_pagerank(links, ranks, n, beta=0.8, iterations=40):\n",
        "    for _ in range(iterations):\n",
        "        contribs = links.join(ranks).flatMap(\n",
        "            lambda x: [(dest, x[1][1] / len(x[1][0])) for dest in x[1][0]]\n",
        "        )\n",
        "        ranks = contribs.reduceByKey(lambda a, b: a + b) \\\n",
        "                       .mapValues(lambda rank: (1 - beta) / n + beta * rank)\n",
        "    return ranks\n",
        "\n",
        "def main():\n",
        "    file_path = \"small.txt\"\n",
        "    sc, links = load_graph(file_path)\n",
        "    ranks, n = initialize_ranks(links)\n",
        "    final_ranks = run_pagerank(links, ranks, n)\n",
        "\n",
        "    top5 = final_ranks.takeOrdered(5, key=lambda x: -x[1])\n",
        "    bottom5 = final_ranks.takeOrdered(5, key=lambda x: x[1])\n",
        "    print(\"-----------Top 5 high PageRank:----\")\n",
        "    for node, score in top5:\n",
        "        print(f\"Node {node}: {score:.6f}\")\n",
        "    print(\"------- Bottom 5 lowest PageRank ----\")\n",
        "    for node, score in bottom5:\n",
        "        print(f\"Node {node}: {score:.6f}\")\n",
        "    sc.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7f70ef",
      "metadata": {
        "id": "ff7f70ef"
      },
      "source": [
        "Whole.txt dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e27cfaf",
      "metadata": {
        "id": "6e27cfaf",
        "outputId": "995b7080-8a14-4725-93cb-3d192b1d9fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 nodes with highest PageRank:\n",
            "Node 263: 0.002020\n",
            "Node 537: 0.001943\n",
            "Node 965: 0.001925\n",
            "Node 243: 0.001853\n",
            "Node 285: 0.001827\n",
            "\n",
            "Bottom 5 nodes with lowest PageRank:\n",
            "Node 558: 0.000329\n",
            "Node 93: 0.000351\n",
            "Node 62: 0.000353\n",
            "Node 424: 0.000355\n",
            "Node 408: 0.000388\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def initialize_ranks(links):\n",
        "    n = links.count()\n",
        "    ranks = links.mapValues(lambda _: 1.0 / n)\n",
        "    return ranks, n\n",
        "\n",
        "def run_pagerank(links, ranks, n, beta=0.8, iterations=40):\n",
        "    for _ in range(iterations):\n",
        "        contribs = links.join(ranks).flatMap(\n",
        "            lambda x: [(dest, x[1][1] / len(x[1][0])) for dest in x[1][0]]\n",
        "        )\n",
        "        ranks = contribs.reduceByKey(lambda a, b: a + b) \\\n",
        "                       .mapValues(lambda rank: (1 - beta) / n + beta * rank)\n",
        "    return ranks\n",
        "def main():\n",
        "    file_path = \"whole.txt\"\n",
        "    sc, links = load_graph(file_path)\n",
        "    ranks, n = initialize_ranks(links)\n",
        "    final_ranks = run_pagerank(links, ranks, n)\n",
        "\n",
        "    top5 = final_ranks.takeOrdered(5, key=lambda x: -x[1])\n",
        "    bottom5 = final_ranks.takeOrdered(5, key=lambda x: x[1])\n",
        "    print(\"---------Top 5 nodes with highest PageRank----\")\n",
        "    for node, score in top5:\n",
        "        print(f\"Node {node}: {score:.6f}\")\n",
        "    print(\"------ Bottom 5 nodes with lowest PageRank ----------\")\n",
        "    for node, score in bottom5:\n",
        "        print(f\"Node {node}: {score:.6f}\")\n",
        "    sc.stop()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}